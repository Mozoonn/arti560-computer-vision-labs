{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1d91397f-18d0-4b28-bed5-cfa96f936874",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6efb726a-2d5b-4ed9-9ca4-c0d74108bfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mozoo\\anaconda3\\envs\\tf\\Lib\\site-packages\\tensorflow\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351cf3b-3f0d-450f-ab17-39c69393825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load CIFAR-10\n",
    "# -----------------------------\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
    "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
    "]\n",
    "\n",
    "# Keep labels as integers (SparseCategoricalCrossentropy)\n",
    "y_train = y_train.squeeze().astype(\"int64\")\n",
    "y_test  = y_test.squeeze().astype(\"int64\")\n",
    "\n",
    "# Convert images to float32\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Data augmentation\n",
    "# -----------------------------\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "], name=\"augmentation\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build MobileNetV2 backbone (pretrained)\n",
    "# -----------------------------\n",
    "mobile_base = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "mobile_base.trainable = False  # freeze first (feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994673f3-5ced-4599-9fda-dc41c20473bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MobileNetV2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resizing_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resizing_2 (\u001b[38;5;33mResizing\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m12,810\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 865ms/step - accuracy: 0.6813 - loss: 0.9105 - val_accuracy: 0.8162 - val_loss: 0.5331 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Full model (preprocess inside model)\n",
    "# -----------------------------\n",
    "mobilenet_model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    layers.Lambda(preprocess_input),          # IMPORTANT: correct for ResNet50V2\n",
    "    mobile_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10)                          # logits\n",
    "], name=\"MobileNetV2\")\n",
    "\n",
    "mobilenet_model.summary()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Compile + Train (frozen backbone)\n",
    "# -----------------------------\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1),\n",
    "]\n",
    "\n",
    "history = mobilenet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6a4bb4-ac38-42e4-8f0f-9401abbdc8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (frozen) test accuracy: 0.8126000165939331\n",
      "MobileNetV2 (frozen) test loss    : 0.5426377654075623\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Test / Evaluate\n",
    "# -----------------------------\n",
    "test_loss, test_acc_r = mobilenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MobileNetV2 (frozen) test accuracy:\", test_acc_r)\n",
    "print(\"MobileNetV2 (frozen) test loss    :\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8646ab-a973-4946-aace-cbfb2b738693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers in backbone: 30 / 154\n",
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 1s/step - accuracy: 0.7125 - loss: 0.8256 - val_accuracy: 0.8222 - val_loss: 0.5170\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m885s\u001b[0m 1s/step - accuracy: 0.7741 - loss: 0.6513 - val_accuracy: 0.8408 - val_loss: 0.4511\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m908s\u001b[0m 1s/step - accuracy: 0.7976 - loss: 0.5823 - val_accuracy: 0.8510 - val_loss: 0.4179\n",
      "MobileNetV2 (fine-tuned) test accuracy: 0.848800003528595\n",
      "MobileNetV2 (fine-tuned) test loss    : 0.4394311010837555\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "#Fine-tune last layers\n",
    "# -----------------------------\n",
    "mobile_base.trainable = True\n",
    "mobilenet_model.trainable = True\n",
    "for layer in mobile_base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Trainable layers in backbone:\", sum(l.trainable for l in mobile_base.layers), \"/\", len(mobile_base.layers))\n",
    "\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = mobilenet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss_ft, test_acc_ft = mobilenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MobileNetV2 (fine-tuned) test accuracy:\", test_acc_ft)\n",
    "print(\"MobileNetV2 (fine-tuned) test loss    :\", test_loss_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1209799-bbe1-42a2-9ced-c27d2b77fbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet frozen test acc: 0.8126000165939331\n",
      "ResNet fine-tuned test acc: 0.848800003528595\n"
     ]
    }
   ],
   "source": [
    "# Collect and compare accuracies (update if you rename variables)\n",
    "results = {\n",
    "    \"ResNet frozen test acc\": float(test_acc_r) if 'test_acc_r' in globals() else None,\n",
    "    \"ResNet fine-tuned test acc\": float(test_acc_ft) if 'test_acc_ft' in globals() else None,\n",
    "}\n",
    "for k,v in results.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54544ab0-216d-4b4f-8aa7-a021046e1e68",
   "metadata": {},
   "source": [
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "ResNet50V2 achieved the highest accuracy, around 91.6%, which is higher than both MobileNetV2 81.2% and the custom CNN.\n",
    "\n",
    "- Which model trained faster?\n",
    "MobileNetV2 also trained faster, since each epoch took less time compared to ResNet50V2\n",
    "\n",
    "- How might the architecture explain the differences?\n",
    "The difference comes from how the models are built. ResNet is much deeper and has way more parameters so it learn more complex features, which is why it usually gets higher accuracy. But because it’s bigger it also takes much longer to train.\n",
    "\n",
    "MobileNet is designed to be lightweight so it has much fewer parameters and uses more efficient layers. That makes it faster but it doesn’t learn features as deeply as ResNet so the accuracy can be lower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
